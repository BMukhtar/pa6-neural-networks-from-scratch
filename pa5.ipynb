{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-4c943a74-89a4-4936-a64a-1f160563a0c3",
    "deepnote_cell_type": "markdown",
    "id": "CBvij-E8ws6i"
   },
   "source": [
    "# Programming Assignment 5: Yelp, Neural Nets, and Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-72aa6fcd-ab1c-45dd-98a5-90cf247739d7",
    "deepnote_cell_type": "markdown",
    "id": "pNH8-FIUSSIC"
   },
   "source": [
    "In this assignment, we'll cap off the first part of the class, which was\n",
    "focused on text processing, NLP tasks like information retrieval and sentiment\n",
    "analysis, and tools like word vectors and Naive Bayes that we can use to solve\n",
    "them.\n",
    "\n",
    "In Week 2/PA2, we learned about\n",
    "Naive Bayes and logistic regression, and how they can be used\n",
    "to solve text classification problems. In Week 5/PA4, we learned about\n",
    "vector semantics, and how we can use word embeddings as a tool to represent\n",
    "language and answer semantic questions.\n",
    "\n",
    "In this assignment, weâ€™ll integrate and expand these ideas to build a new and\n",
    "powerful tool for your NLP toolkit: a simple neural network!\n",
    "\n",
    "Neural networks (and in particular, neural networks using semantic word\n",
    "embeddings) are extremely useful and powerful tools that are behind some\n",
    "really impressive advances in NLP over the last few years. You'll see plenty of\n",
    "them in future NLP classes (if you choose to take them), but hopefully this\n",
    "assignment will give you a first taste of how they work and what you can do\n",
    "with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Before we do anything else, let's quickly check that you're running the correct\n",
    "version of Python and are in the right environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs124\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell complains, it means that you're using the wrong environment\n",
    "or Python version!\n",
    "\n",
    "If so, please exit this notebook, kill the notebook server with CTRL-C, and\n",
    "try running\n",
    "\n",
    "$ conda activate cs124\n",
    "\n",
    "then restarting your notebook server with\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "If that doesn't work, you should go back and follow the installation\n",
    "instructions in PA0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-b76b2fef-04ed-469b-8893-a3daa5c6502d",
    "deepnote_cell_type": "markdown",
    "id": "umaY6CwjwnSt"
   },
   "source": [
    "## Part 1: Why not Logistic Regression?\n",
    "\n",
    "Before we build a simple neural network, let's take a brief look back at\n",
    "logistic regression (from PA2).\n",
    "\n",
    "Recall that the output of a logistic regression classifier is given by:\n",
    "\n",
    "$y = \\sigma(XW + b)$\n",
    "\n",
    "Where $W$ was a vector of shape (num_features,), $X$ was a matrix of shape\n",
    "(num_examples, num_features), $b$ was a scalar added to every element of the\n",
    "vector $XW$, and y was a vector of shape (num_examples,), and $\\sigma()$ is the\n",
    "sigmoid function.\n",
    "\n",
    "We showed in PA2 that this approach can work really well for text\n",
    "classification. However, it has a significant weakness that we need to be\n",
    "aware of.\n",
    "\n",
    "To see that, let's bring out our logistic regression classifier again and apply\n",
    "it to a toy dataset that we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-9f3bde15-181e-44a8-88e9-5fc68598900e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 888,
    "execution_start": 1619508614561,
    "id": "KB6ZUXp441N6",
    "output_cleared": true,
    "source_hash": "202f447a"
   },
   "outputs": [],
   "source": [
    "# Import all of the packages we'll need for the rest of the assignment\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "from util import (generate_2d_xor_dataset, plot_2d_dataset_points,\n",
    "plot_points_with_classifier_predictions, check_forward_pass,\n",
    "check_backward_pass, load_embeddings, load_dataset, check_examples_to_array,\n",
    "check_forward_pass_yelp, check_backward_pass_yelp, evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-3daf0648-9e59-4496-a81a-dcddd5112911",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1619508616449,
    "id": "G5GOoYtrwzmR",
    "output_cleared": true,
    "source_hash": "1d785f64",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Construct an example dataset for us to classify\n",
    "X, y = generate_2d_xor_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-2cd87dc4-71c6-4d31-ab8a-0614defa67b3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 337,
    "execution_start": 1619508620002,
    "id": "fkKWlBvdw4Dk",
    "outputId": "7de01a83-6377-468b-8b1d-e90655b08b44",
    "output_cleared": true,
    "source_hash": "d1b799c0",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot our example dataset\n",
    "plot_2d_dataset_points(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-631ebd22-1b38-4255-bc25-8b716ed9eba8",
    "deepnote_cell_type": "markdown",
    "id": "IqQMOcX1xLWj"
   },
   "source": [
    "This is what our data looks like! It's a bit of an unusual-looking\n",
    "distribution. However, datasets with these sorts of properties actually appear\n",
    "quite often in real life. To give an example of such a real-life situation,\n",
    "consider the case of doing sentiment analysis, but in the presence of\n",
    "sarcasm.\n",
    "\n",
    "We can plot a single point for each sentence, where the $x$ value represents\n",
    "the average sentiment score of the words in the sentence, and the $y$ value\n",
    "represents the amount of sarcasm detected in the sentence (imagine we have\n",
    "some way to measure this).\n",
    "\n",
    "The data is labeled in two classes: class 0 (blue) means a sentence's true\n",
    "sentiment is negative, while class 1 (red) means a sentence's true sentiment\n",
    "is positive.\n",
    "\n",
    "With that in mind, we can imagine why we might see a pattern like this. Sarcasm\n",
    "tends to invert the meaning of a sentence. For example, the sentence\n",
    "\"Woooooow, what a FANTASTIC day, I hope it never ends.\", by itself appears to\n",
    "express positive sentiment. If we took the average sentiment score of the words\n",
    "in the sentence, it would probably be positive, because of the highly\n",
    "positive word \"fantastic\". But if we also knew that the speaker was expressing\n",
    "a lot of sarcasm, then that would imply that the sentence's true sentiment\n",
    "is the opposite: Negative.\n",
    "\n",
    "This is why a high average sentiment score and low sarcasm score corresponds\n",
    "to a positive true sentiment label, but a high average sentiment score and a\n",
    "high sarcasm score corresponds to a negative true sentiment label!\n",
    "\n",
    "NOTE: In case you're curious, this dataset was actually generated from 4 normal\n",
    "distributions with different means and unit variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, consider how we might attempt to do binary classification on this\n",
    "dataset. Or to think about it geometrically, how we could draw a line to\n",
    "separate the blue points from the red points. Visually, it doesn't seem\n",
    "difficult to imagine such a line (in fact, there's an\n",
    "infinite number of lines that we could draw to correctly classify all the\n",
    "points).\n",
    "\n",
    "However, this dataset is actually an example of a problem that logistic\n",
    "regression (and other methods like it) can't solve! (See Ch 7.2 of Jurafsky\n",
    "and Martin for more info on this problem, often called the XOR problem)\n",
    "\n",
    "Don't believe us? Let's see what happens when we try to apply logistic\n",
    "regression to this data.\n",
    "\n",
    "There's no need to implement it again, because we already got plenty of\n",
    "practice with that in PA2. This time, we'll use Scikit-learn's pre-written\n",
    "version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-8ff93d54-4077-4cbf-b75a-197eb6b55e7e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619496574863,
    "id": "XKJZ9KXixF7p",
    "output_cleared": true,
    "source_hash": "f1714c1d",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression classifier on our data\n",
    "logistic_regression_classifier = LogisticRegression().fit(X, np.squeeze(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-c9c30277-edee-4fd5-9dba-c56a1047b774",
    "deepnote_cell_type": "markdown",
    "id": "rt9FQPpnxONN"
   },
   "source": [
    "And now, let's see what our trained classifier predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-d2f78405-7d19-41ba-82a1-ad7edc95fa7d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 299,
    "execution_start": 1619496579999,
    "output_cleared": true,
    "source_hash": "678180cf",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "plot_points_with_classifier_predictions(X, y, logistic_regression_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-3d43f862-803d-4b1b-90b7-38488568fceb",
    "deepnote_cell_type": "markdown",
    "id": "zsKNS6NzxXja"
   },
   "source": [
    "In this plot, we are showing the points with their true labels as colored dots\n",
    "(just like the previous plot), but we've overlaid the predictions of the\n",
    "logistic regression classifier as the colored regions. The blue shaded region\n",
    "is the area where the classifier is predicting that points are class 0 and a\n",
    "negative true sentiment. And the red area is where it predicts that points\n",
    "are class 1 and of positive true sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-90581dcc-8c15-455b-a4af-bb7a6e697d4a",
    "deepnote_cell_type": "markdown",
    "id": "hz9RWqkZxYNN"
   },
   "source": [
    "So, what just happened?? This prediction is way off! Is Scikit-Learn's\n",
    "logistic regression classifier broken? It seemed to work so well in PA2...\n",
    "\n",
    "The answer is actually no: it's working correctly! The reason why it fails\n",
    "to classify the points correctly here has to do with a fundamental limitation\n",
    "of logistic regression. It's a __linear__ classifier, so it can only make\n",
    "predictions by drawing a linear boundary. Remember that our\n",
    "logistic regression equation is $y = \\sigma(XW + b)$. In this case, the\n",
    "decision boundary is given by $XW + b = 0$.\n",
    "\n",
    "Can you see why this is true, and why logistic regression is a __linear model__,\n",
    "even though sigmoid is a non-linear function? Trying picking a particular choice\n",
    "of $W$ and $b$ and drawing the line given by $XW + b = 0$. Then try plugging\n",
    "in some different values of $X$. You should find that all points $X$ on one\n",
    "side of that line will be predicted as class 0, and all points on the other as\n",
    "class 1.\n",
    "\n",
    "One interesting thing is that linear models work surprisingly well in practice,\n",
    "as we saw in PA2! It turns out that drawing a straight line is often enough to\n",
    "get you a pretty good answer to many problems, especially if you're working\n",
    "in a high-dimensional space (i.e. you have a lot of input features).\n",
    "\n",
    "However, as we can see here there are some problems that linear models just\n",
    "can't deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-4a4987a3-fbda-428a-9848-f5d453d018c9",
    "deepnote_cell_type": "markdown",
    "id": "iyE8LP0-xdS7"
   },
   "source": [
    "### One Solution: A Simple Neural Network\n",
    "\n",
    "One way to think about neural networks is as an extension and improvement on\n",
    "logistic regression that addresses the above limitation. They do this by\n",
    "essentially just stacking a bunch of logistic regression classifiers together!\n",
    "\n",
    "What do we mean by this?\n",
    "\n",
    "Neural networks are made up of a series of layers. Each of those layers looks\n",
    "almost exactly like a logistic regression classifier. They can be written as:\n",
    "\n",
    "$a_l = f(a_{l-1}W_l + b_l)$\n",
    "\n",
    "Where $a_l$ represents the output of layer $l$ and $f()$ is a non-linear\n",
    "function called the __activation function__. In the case of logistic regression,\n",
    "you can think of sigmoid() as the activation function.\n",
    "\n",
    "To calculate the output of each layer, we take an input ($a_{l-1}$, possibly\n",
    "the output from the previous layer), multiply it by a\n",
    "matrix of weights ($W_l$) and add a bias ($b_l$), then apply a non-linear\n",
    "activation function ($f()$) to it. The weights and bias can be learned, just\n",
    "like in logistic regression.\n",
    "\n",
    "If we take a single layer, set $f() = \\text{sigmoid}() = \\sigma()$, make our\n",
    "layer input ($a_0$) our input data $X$, and choose the dimensions of $W_1$ and\n",
    "$b_1$ so that our layer output ($a_1$) is of shape\n",
    "(num_examples,), that's just logistic regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-e68b4435-28f0-4470-aade-af68e4565ff5",
    "deepnote_cell_type": "markdown",
    "id": "z03RV1HCxd_Z"
   },
   "source": [
    "Two important things to note:\n",
    "\n",
    "1. In logistic regression (one \"layer\"), our input is always the input data,\n",
    "and our output is always a prediction score. So the shapes of the inputs and\n",
    "outputs (and therefore the shapes of $W$ and $b$) were pre-determined. However,\n",
    "if we're stacking a bunch of these layers together we have some flexibility to\n",
    "choose the inputs and output shapes, so long as the first layer's\n",
    "input shape matches the input data, the last layer's output shape is the shape\n",
    "of the prediction scores, and all the input and output shapes of the layers\n",
    "in-between match up. This is one way that we can tune and modify our models.\n",
    "\n",
    "2. The key reason why \"stacking\" like this is helpful has to do with the fact\n",
    "that $f()$ is always a non-linear function. We'll see why this is\n",
    "in a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-846c7d1c-b3b6-40d0-b562-a5cf17cefea7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### The Math\n",
    "\n",
    "The next thing you'll need to do is implement a multi-layer neural network!\n",
    "Doing that will require writing some equations. First, consider the equations\n",
    "to calculate the output of each layer in the network\n",
    "(i.e. $a_1, a_2, a_3, ...$) from the first layer all the way to the predicted\n",
    "scores. These are called the __forward pass__:\n",
    "\n",
    "Define $a_l$ as the output after the non-linearity in layer $l$ and $z_l$ as\n",
    "the output before the non-linearity in layer $l$. The neural network forward\n",
    "pass equations are then as follows:\n",
    "\n",
    "$z_1 = X W_1 + b_1$\n",
    "\n",
    "$a_1 = \\sigma(z_1)$\n",
    "\n",
    "$z_2 = a_1 W_2 + b_2$\n",
    "\n",
    "$a_2 = \\sigma(z_2)$\n",
    "\n",
    "$z_3 = a_2 W_3 + b_3$\n",
    "\n",
    "$a_3 = \\sigma(z_3)$\n",
    "\n",
    "Recall from the modules/reading that we can train a neural network in the same\n",
    "way as logistic regression, by defining a cost/loss function that expresses\n",
    "how far off the predictions are from the correct answers, and then using\n",
    "gradient descent to update the weights and biases to minimize this error.\n",
    "\n",
    "For this problem, we use the binary cross entropy\n",
    "cost function (the same as for logistic regression!), defined as:\n",
    "\n",
    "$J = - (y log(a_3) + (1 - y) log(1 - a_3))$\n",
    "\n",
    "To do our gradient descent, we need to somehow compute the gradients of the loss\n",
    " with respect to for all the weights and biases. We showed what these gradients\n",
    " look like for logistic regression in the modules and Ch. 5 of the textbook, but\n",
    " now we have multiple \"layers\" of logistic regressions, and we need to calculate\n",
    " the gradients for the weights and biases in every single layer.\n",
    "\n",
    "We showed in the modules how you can do this with a computational graph and\n",
    "using the chain rule. This is called the __backward pass__ or backpropagation\n",
    "(backprop).\n",
    "\n",
    "In the module/slides (see slide 79), we showed that $\\frac{dJ}{dz_3} = a_3 - y$.\n",
    "Using this fact, the chain rule, and the equations above for the forward pass,\n",
    "we can calculate the gradients of all of our parameters:\n",
    "\n",
    "$\\frac{dJ}{dz_3} = a_3 - y$\n",
    "\n",
    "$\\frac{dJ}{dW_3} = \\frac{1}{m} (a_{2}^T \\frac{dJ}{dz_3})$\n",
    "\n",
    "$\\frac{dJ}{db_3} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_3})_i$\n",
    "\n",
    "$\\frac{dJ}{da_2} = \\frac{dJ}{dz_3} W_3^T$\n",
    "\n",
    "$\\frac{dJ}{dz_2} = \\frac{dJ}{da_2} \\circ a_2 \\circ (\\mathbb{1}-a_2)$\n",
    "\n",
    "$\\frac{dJ}{dW_2} = \\frac{1}{m} (a_{1}^T \\frac{dJ}{dz_2})$\n",
    "\n",
    "$\\frac{dJ}{db_2} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_2})_i$\n",
    "\n",
    "$\\frac{dJ}{da_1} = \\frac{dJ}{dz_2} W_2^T$\n",
    "\n",
    "$\\frac{dJ}{dz_1} = \\frac{dJ}{da_1} \\circ a_1 \\circ (\\mathbb{1}-a_1)$\n",
    "\n",
    "$\\frac{dJ}{dW_1} = \\frac{1}{m} (X^T \\frac{dJ}{dz_1})$\n",
    "\n",
    "$\\frac{dJ}{db_1} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_1})_i$\n",
    "\n",
    "__NOTE:__ Note that to compute the gradients we use to update our weights and\n",
    "biases, we take the mean of the gradients over all $m$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-208bb6c1-5ebc-4d48-bef5-4c6edffb2754",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### TODO: Implementing the Network\n",
    "\n",
    "Now we have all the information we need to implement a simple neural network\n",
    "that consists of three logistic regression layers \"stacked together\"!\n",
    "\n",
    "If the code below seems intimidating at first, don't worry! On closer\n",
    "inspection, it should look very familiar: it's only a slight modification of\n",
    "the code you wrote in PA 2 for your logistic regression classifier.\n",
    "\n",
    "The most complicated part will probably be computing the gradients in\n",
    "backward_pass(), but for that, you can refer closely to the equations we\n",
    "wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-6cd15ba1-9492-422b-935a-be1b66bbe234",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 42,
    "execution_start": 1619508800186,
    "id": "gbVnL9MBxTpK",
    "output_cleared": true,
    "source_hash": "c8678370"
   },
   "outputs": [],
   "source": [
    "class StackedLogisticRegressionNetwork:\n",
    "    def __init__(self, input_size: int, layer_1_size: int, layer_2_size: int,\n",
    "                 seed: int = 1):\n",
    "        \"\"\"\n",
    "        TODO: Initialize the weights and biases of the 3 layers. You should\n",
    "        initialize the weights randomly with initialize_weights() for each\n",
    "        layer, and initialize the biases to zero for each layer.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input (number of input features/\n",
    "                             number of dimensions of the input points).\n",
    "            layer_1_size (int): The number of hidden units in the first hidden\n",
    "                                layer.\n",
    "            layer_2_size (int): The number of hidden units in the second hidden\n",
    "                                layer.\n",
    "            seed (int): Random seed to use when initializing weights.\n",
    "\n",
    "        HINT: You should use the initialize_weights() method that we give you to\n",
    "        initialize the weights for each layer (although you'll need to figure\n",
    "        out what the correct shapes are).\n",
    "\n",
    "        HINT: The shape of the bias for each layer should be (1, layer_size).\n",
    "        You should use np.zeros() to initialize your biases to zero.\n",
    "\n",
    "        WARNING: Do NOT change the order in which the weights are initialized\n",
    "        below, as this may cause your random initialization to be different from\n",
    "        ours when running tests.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        self.W_1 = None\n",
    "        self.b_1 = None\n",
    "        self.W_2 = None\n",
    "        self.b_2 = None\n",
    "        self.W_3 = None\n",
    "        self.b_3 = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE ENDS HERE ###########################\n",
    "        \"\"\"\n",
    "\n",
    "        self.variables = {\n",
    "          \"W_1\": self.W_1,\n",
    "          \"b_1\": self.b_1,\n",
    "          \"W_2\": self.W_2,\n",
    "          \"b_2\": self.b_2,\n",
    "          \"W_3\": self.W_3,\n",
    "          \"b_3\": self.b_3\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_weights(num_inputs: int, num_outputs: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Initialize the weights in a specific way to help gradient descent\n",
    "        converge better (Glorot initialization).\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): The number of inputs to the layer.\n",
    "            num_outputs (int): The number of outputs (units) in the layer.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A fully initialized NumPy array of weights, of shape\n",
    "            (num_inputs, num_outputs)\n",
    "        \"\"\"\n",
    "        bound = np.sqrt(6 /(num_inputs + num_outputs))\n",
    "        return np.random.uniform(-bound, bound, (num_inputs, num_outputs))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The sigmoid function, exactly the same as in PA2. NOTE: Applied\n",
    "        element-wise to each element in the array x.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): A NumPy array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of the sigmoid function applied to each element\n",
    "                        of x. Shape is the same as x.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward_pass(self, X: np.ndarray) ->\\\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        TODO: Calculate the outputs of each of the three layers in the model\n",
    "        using the inputs, weights, and biases.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The NumPy array of input examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple of three NumPy\n",
    "                arrays, corresponding to the output (after activation) of the\n",
    "                first layer ($a_1$), second layer ($a_2$), and third/output\n",
    "                layer ($a_3$).\n",
    "\n",
    "\n",
    "        HINT: Your goal here is to essentially translate the forward pass\n",
    "        equations above from math into Python/NumPy.\n",
    "\n",
    "        HINT: Your output a_1 should look exactly like the output of a logistic\n",
    "        regression classifier on X. Your output a_2 should look exactly like\n",
    "        the output of another logistic regression classifier on the input a_1.\n",
    "        And same again for a_3. Note that you should use the appropriate\n",
    "        weights and biases for each part though.\n",
    "\n",
    "        HINT: Don't forget to apply the self.sigmoid() between each layer!\n",
    "\n",
    "        HINT: Your output a_3 should be of shape (num_examples, 1), because it\n",
    "        represents the final predictions of our neural network for each example.\n",
    "        For each example, it contains a single number between 0 and 1 for each\n",
    "        example. If it is close to 1, the network thinks the example is probably\n",
    "        class 1, and if it is close to 0, the network thinks the example is\n",
    "        probably class 0.\n",
    "\n",
    "        NOTE: We will need to use these 3 values later when computing the\n",
    "        gradients in backward_pass().\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        a_1 = None\n",
    "        a_2 = None\n",
    "        a_3 = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE ENDS HERE ###########################\n",
    "        \"\"\"\n",
    "        return a_1, a_2, a_3\n",
    "\n",
    "    def backward_pass(self,\n",
    "                      X : np.ndarray,\n",
    "                      y : np.ndarray,\n",
    "                      a_1: np.ndarray,\n",
    "                      a_2: np.ndarray,\n",
    "                      a_3: np.ndarray\n",
    "                      ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        TODO: Use the examples, labels, and model outputs to compute the\n",
    "        gradient for each of the weights and biases.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The NumPy array of input examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "            y (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X. Has shape\n",
    "                            (num_examples, 1).\n",
    "            a_1 (np.ndarray): The output of the first logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "            a_2 (np.ndarray): The output of the second logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "            a_3 (np.ndarray): The output of the third logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: A dictionary of the gradients computed for\n",
    "                                    each weight and bias parameter. The keys\n",
    "                                    are strings, the values are NumPy arrays\n",
    "                                    (see the names of the keys below).\n",
    "\n",
    "        HINT: Your goal here is essentially to translate the backprop equations\n",
    "        (the equations for the gradients) below into NumPy calculations.\n",
    "\n",
    "        HINT: One great thing to use as a sanity-check is that your gradient for\n",
    "        each parameter should be the same shape as the parameter itself! For\n",
    "        example, your gradient dW_3 should be the exact same shape as self.W_3.\n",
    "\n",
    "        HINT: The first dimension of X and y is the number of examples/data\n",
    "        points. Note that the shape of the gradients should not depend on the\n",
    "        number of examples, so this dimension should be averaged out or\n",
    "        otherwise \"cancelled out\" when computing each gradient.\n",
    "\n",
    "        HINT: When multiplying matrices, remember that you can either use\n",
    "        np.matmul(), or the \"@\" operator, both do the same thing.\n",
    "\n",
    "        HINT: You will probably want to use np.mean() to do some averaging.\n",
    "        Remember that you can pass a \"axis\" argument to only average along a\n",
    "        single dimension.\n",
    "\n",
    "        For example, if we wanted to average an array A of shape (10, 20) along\n",
    "        the second dimension, we could call np.mean(A, axis=1). This will return\n",
    "        an array of shape (10,). If we want to keep the second dimension of\n",
    "        length 1, we can use keepdims=True. So np.mean(A, axis=1, keepdims=True)\n",
    "        would return an array of shape (10, 1).\n",
    "\n",
    "        HINT: You'll probably find it useful to compute:\n",
    "          - dz_3\n",
    "          - da_2\n",
    "          - dz_2\n",
    "          - da_1\n",
    "          - dz_1\n",
    "          as you do your calculations.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        dW_3 = None\n",
    "        db_3 = None\n",
    "        dW_2 = None\n",
    "        db_2 = None\n",
    "        dW_1 = None\n",
    "        db_1 = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE ENDS HERE ###########################\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the gradients\n",
    "        gradients = {\n",
    "          \"W_3\": dW_3,\n",
    "          \"b_3\": db_3,\n",
    "          \"W_2\": dW_2,\n",
    "          \"b_2\": db_2,\n",
    "          \"W_1\": dW_1,\n",
    "          \"b_1\": db_1\n",
    "        }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_hat : np.ndarray,\n",
    "             y : np.ndarray,\n",
    "             epsilon: float=1e-16) -> float:\n",
    "        \"\"\"\n",
    "        Computes the average binary cross-entropy loss over all examples. This\n",
    "        is the same loss function we implemented for logistic regression in PA2.\n",
    "\n",
    "        Args:\n",
    "            y_hat (np.ndarray): The NumPy array of our prediction scores. This\n",
    "                                is the same as the output a_3 from\n",
    "                                forward_pass(). It is of shape\n",
    "                                (num_examples, 1), and\n",
    "                                each element is between 0.0 and 1.0.\n",
    "            y (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X. Has shape\n",
    "                            (num_examples, 1). Each element is either 0.0 or\n",
    "                            1.0.\n",
    "            epsilon (float): A tiny value used to clip y_hat so that it is never\n",
    "                             accidentally rounded to 1.0 or 0.0, causing\n",
    "                             the log to become undefined.\n",
    "\n",
    "        Returns:\n",
    "            float: The average binary cross-entropy loss over the num_examples\n",
    "                    examples.\n",
    "        \"\"\"\n",
    "        # We clip the predictions so they can never become exactly 0 or 1.\n",
    "        # This would cause the log of the predictions to be undefined.\n",
    "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "        return -1 * np.mean(y * np.log(y_hat) + (1.0 - y) * np.log(1.0 - y_hat))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO: Convert the prediction scores (output a_3 in self.forward_pass())\n",
    "        to binary predictions.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The NumPy array of input examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array of shape (num_examples, 1) containing the\n",
    "                        predicted class for each example. Each element should\n",
    "                        be either 0 or 1.\n",
    "\n",
    "        HINT: The predictions scores will be numbers between 0 and 1, for\n",
    "        example a prediction score of 0.8 means that the neural network thinks\n",
    "        there is an 80% chance it is in class 1 and a 20% chance that it is in\n",
    "        class 0. Your job is to take this score and convert it to a\n",
    "        black-and-white guess (exactly 0 or 1). As an example, if the prediction\n",
    "        score is 0.51, the prediction should be 1. If it is 0.49, the prediction\n",
    "        should be 0.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        predictions = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE ENDS HERE ###########################\n",
    "        \"\"\"\n",
    "        return predictions\n",
    "\n",
    "    def train(self,\n",
    "              X_train: np.ndarray,\n",
    "              y_train: np.ndarray,\n",
    "              learning_rate: float = 0.01,\n",
    "              num_epochs: int = 1000,\n",
    "              print_every: int = 5,\n",
    "              verbose: bool=True):\n",
    "        \"\"\"\n",
    "        Trains the neural network using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): The NumPy array of training examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "            y_train (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X_train. Has shape\n",
    "                            (num_examples, 1). Each element is either 0.0 or\n",
    "                            1.0.\n",
    "            learning_rate (np.ndarray): Step size parameter for gradient\n",
    "                                descent.\n",
    "            num_epochs (int): The number of epochs (full passes through the\n",
    "                                provided training data) to train the model for.\n",
    "            print_every (int): Number of epochs to wait between printouts of the\n",
    "                                training loss and accuracy during training.\n",
    "            verbose (bool): Whether to print any training output.\n",
    "\n",
    "        This code should look very familiar to you from PA2. It's pretty much\n",
    "        exactly the same code we used to train our logistic regression model,\n",
    "        except this time you implemented the part that computes the gradients!\n",
    "        \"\"\"\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            a_1, a_2, a_3 = self.forward_pass(X_train)\n",
    "            gradients = self.backward_pass(X_train, y_train, a_1, a_2, a_3)\n",
    "            # Update the weights with gradient descent\n",
    "            for name, grad in gradients.items():\n",
    "                self.variables[name] -= learning_rate * grad\n",
    "            # Occasionally print out the training loss and accuracy\n",
    "            if verbose and epoch % print_every == 0:\n",
    "                loss = self.loss(a_3, y_train)\n",
    "                accuracy = np.mean(self.predict(X_train) == y_train)\n",
    "                print(\"Epoch: {} | Training Loss: {} | Training Accuracy: {}\".format(\n",
    "                    epoch, loss, accuracy))\n",
    "        print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-a4aaabdf-2ea8-4aeb-9790-5860360bf590",
    "deepnote_cell_type": "markdown",
    "id": "GpkFkUMy0bmt"
   },
   "source": [
    "Once you've implemented the code for forward_pass() and backward_pass() above,\n",
    "you can check your work by running the tests below!\n",
    "\n",
    "__WARNING:__ The tests below are just sanity checks, they do __NOT__ provide\n",
    "a guarantee that your implementation is 100% correct. We encourage you to\n",
    "do additional checking and testing to convince yourself that everything\n",
    "is working correctly. You can see how the checks are implemented in util.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-094cc43f-9832-4261-87f3-d127c4fdccd0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1619497088935,
    "output_cleared": true,
    "source_hash": "835db475",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "check_forward_pass(StackedLogisticRegressionNetwork, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-37cadeaa-4149-42af-b88c-fc901c5de10b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1619497090659,
    "output_cleared": true,
    "source_hash": "16996923",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "check_backward_pass(StackedLogisticRegressionNetwork, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-12f5fe45-a38d-44cb-a060-d9fbba2be4f9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "If you passed the two tests above, congratulations! In all likelihood, you have\n",
    "just written a working neural network! Now for the final test:\n",
    "__training and testing it__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-84f496e5-aa22-4399-ae1d-35282949c7b7",
    "deepnote_cell_type": "markdown",
    "id": "kR6BXTpzxmTn"
   },
   "source": [
    "Just like our logistic regression classifier in PA2, we can train our model\n",
    "with gradient descent. You can check out the train() function to see how it\n",
    "works (basically exactly the same as in PA2).\n",
    "\n",
    "The only difference is that because we've stacked 3 logistic regressions\n",
    "together, we have 3 times the numbers of weights and biases, and computing the\n",
    "gradients for them is a little trickier due to how they stack on top of one\n",
    "another. However, you did all of that in backward_pass()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-3b6b1948-b5c8-44e3-9d8b-793dfbb18bba",
    "deepnote_cell_type": "markdown",
    "id": "c_BMCMsSxpit"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-7f172fb5-0575-4c07-9dd7-e81f240701ae",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2480,
    "execution_start": 1619497092993,
    "id": "Hqx5hPqCxwUT",
    "outputId": "c5507e9d-1875-4f51-ab99-aafe94798f9a",
    "output_cleared": true,
    "source_hash": "9c83eb1",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# We choose the size of the 2nd and 3rd layers to be 10. More on this below.\n",
    "neural_network_classifier = StackedLogisticRegressionNetwork(2, 10, 10)\n",
    "neural_network_classifier.train(X, y,\n",
    "                                learning_rate=0.1,\n",
    "                                num_epochs=10000,\n",
    "                                print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-d7b10a16-e4c4-44b4-86fe-57254bdf6b90",
    "deepnote_cell_type": "markdown",
    "id": "HutayYClxrk5"
   },
   "source": [
    "Now let's plot the resulting predictions, just like we did with the logistic\n",
    "regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-3f2565ba-c19c-4505-8a7c-0147727c9bab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2700,
    "execution_start": 1619497097594,
    "id": "RdfY6_O_x1r8",
    "output_cleared": true,
    "source_hash": "ed6a4ad4",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "plot_points_with_classifier_predictions(X, y, neural_network_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-db94a56d-a2e4-4cce-afc8-4edb1ac9697c",
    "deepnote_cell_type": "markdown",
    "id": "PLwgtmy2x4Bv"
   },
   "source": [
    "Wow! This worked a lot better than logistic regression!\n",
    "\n",
    "In particular, note that it is not subject to the same limitation as logistic\n",
    "regression: the boundary is no longer linear!\n",
    "\n",
    "It's the fact that the boundary is not linear that allows us to correctly\n",
    "classify this dataset. No single straight line could correctly put all the red\n",
    "points on one side and all blue ones on the other side, but a curved line can!\n",
    "\n",
    "You might be wondering: Why is this model (which is just three stacked logistic\n",
    " regression layers) non-linear, when a single logistic regression layer is\n",
    "linear?\n",
    "\n",
    "The answer has to do with what happens when we have multiple layers together,\n",
    "each of which includes a non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-bb53d012-d335-40f3-8f76-e660f15afc03",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Non-linear Magic\n",
    "\n",
    "Let's consider a slightly different model:\n",
    "\n",
    "Consider a 2-layer neural network where __we only apply the sigmoid after the\n",
    "final layer__, but not the first layer.\n",
    "\n",
    "The equation of the first layer is:\n",
    "\n",
    "$z_1 = X W_1 + b_1$\n",
    "\n",
    "The equation of the second layer is:\n",
    "\n",
    "$z_2 = \\sigma(z_1 W_2 + b_2) = \\sigma((X W_1 + b_1) W_2 + b_2)\n",
    "= \\sigma(X W_1 W_2 + b_1 W_2 + b_2)$\n",
    "\n",
    "Note that this is still a linear model! The decision boundary will still be a\n",
    "straight line (do you see why?).\n",
    "\n",
    "What happens if we stack two layers, __but apply a non-linear sigmoid function\n",
    "between every layer__?\n",
    "\n",
    "The equation of the first layer is:\n",
    "\n",
    "$z_1 = \\sigma(X W_1 + b_1)$\n",
    "\n",
    "The equation of the second layer is:\n",
    "\n",
    "$z_2 = \\sigma(z_1 W_2 + b_2) = \\sigma(\\sigma(X W_1 + b_1) W_2 + b_2)$\n",
    "\n",
    "This is a non-linear model! The decision boundary can't be expressed as a linear\n",
    "function. Do you see why (hint: compare what is inside the outer sigmoid between\n",
    "the first case and the second case)?\n",
    "\n",
    "It's this ability to create a non-linear decision boundary by stacking\n",
    "multiple layers (with non-linear functions in between) that makes neural\n",
    "networks more powerful than logistic regression, and potentially more useful\n",
    "for solving challenging problems that can't be separated linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-54c1f607-7082-4804-ba90-4b9f53452237",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Some Final Thoughts on Part 1\n",
    "\n",
    "__Layer Sizes__\n",
    "\n",
    "You can see that when training our model, we chose our input size to be 2, and\n",
    "our hidden layer sizes to both be 10.\n",
    "\n",
    "The input size (size of the first logistic regression layer)\n",
    "we didn't have any choice about, because we have to match whatever our input\n",
    "data X looks like. In this case, we had 2-dimensional inputs, so our input size\n",
    "is 2.\n",
    "\n",
    "But for the second and third logistic regression layers, we're free\n",
    "to choose any size we'd like! Choosing larger sizes means more parameters, which\n",
    "means our boundary can potentially become more complex and fancy, which we might\n",
    "need to solve harder problems. But it also means more numbers to crunch when\n",
    "calculating the gradients, which can make training take longer. In practice,\n",
    "there's no strict rule for choosing the perfect layer sizes.\n",
    "Usually you just have to experiment with some different choices\n",
    "and see what works well!\n",
    "\n",
    "__[Try this:]__ We'd encourage you to go back and try changing the layer sizes\n",
    "and see what happens to your model's training and predictions! What happens\n",
    "when you choose super tiny layer sizes (1, 2)? What about super large ones\n",
    "(100, 1000, 10000)?\n",
    "\n",
    "Finally, note that no matter what sizes we choose for the second and third\n",
    "layers, the final output has to be a single number (prediction score) for each\n",
    "example in X, which is why the last layer's size is determined for us.\n",
    "\n",
    "__Non-linear functions__\n",
    "\n",
    "In our example model we stacked 3 logistic regressions together, and logistic\n",
    "regression uses the sigmoid function on its output. That's why we ended up with\n",
    "3 sigmoid functions sandwiched between the layers of our network.\n",
    "\n",
    "The final sigmoid (after the last layer) is mandatory, for the same reason\n",
    "that we needed it in our original logistic regression classifier. We want our\n",
    "outputs to all be probabilities in the range (0, 1), and the sigmoid function\n",
    "lets us squeeze them down into this range.\n",
    "\n",
    "However, for the \"inner\" sigmoids, in-between the hidden layers, we don't\n",
    "necessarily have to use sigmoids at all! The reasoning above about having a\n",
    "non-linear boundary would apply equally well if we used a different non-linear\n",
    "function instead of sigmoid (you can look back and verify that every step above\n",
    "still makes sense if we replace $\\sigma$ with any other non-linear\n",
    "function).\n",
    "\n",
    "In practice, most \"real\" neural networks don't use sigmoid (for reasons we won't\n",
    "go into, but that you'll learn more about in future deep learning classes). Instead, they use\n",
    "other simple non-linear functions, most commonly ReLU, which we mentioned in\n",
    "the modules and which we'll use in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-d61e9093-b673-4114-a264-fe7fd937cf65",
    "deepnote_cell_type": "markdown",
    "id": "C9v6cGlr90hL"
   },
   "source": [
    "## Part 2: Yelp Review Classification\n",
    "\n",
    "Now that we've seen what neural networks can do, it's time to apply them to\n",
    "some real language data! More specifically, we'll return to sentiment analysis,\n",
    "which you should be very familiar with from PA2. However, now we'll see how a\n",
    "neural network fares on this task.\n",
    "\n",
    "More specifically, our goal will be to take in data in the form of Yelp reviews\n",
    "of various businesses and predict, just from the text of the review itself,\n",
    "whether the reviewer's opinion was positive or negative!\n",
    "\n",
    "To do that, we need to take one last detour to the world of vector semantics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-5297a1aa-23c6-44c2-9648-4fe198273d16",
    "deepnote_cell_type": "markdown",
    "id": "KBSmYXvU5hUu"
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "You'll recall from PA4 that semantic word vectors, or word embeddings, are a\n",
    "very useful and powerful way to represent words.\n",
    "We showed how you can look at the geometric properties of these vectors and use\n",
    "them to show relationships between words, like analogies or word similarity.\n",
    "\n",
    "That's only a small part of what makes embeddings so useful and interesting!\n",
    "Embeddings are a key building block for solving all sorts of other language and\n",
    "NLP-related problems. Once we've transformed words and text into numerical\n",
    "values, that lets us apply all sorts of tools from our machine learning toolbox\n",
    "to the problem we're interested in.\n",
    "\n",
    "Neural networks are one such category of machine learning tools (and an\n",
    "extremely popular one). Recall how our simple neural network above was able to\n",
    "take 2D points (in other words, 2-dimensional vectors of numbers) as inputs, and\n",
    "output predictions. If we could just convert our text or words into a similar\n",
    "format (vectors of numbers), then we could feed them into a neural\n",
    "network in the same way.\n",
    "\n",
    "Word embeddings are just what we need to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-3e34bd9f-9e46-4cac-9a89-ef0b450c1ebd",
    "deepnote_cell_type": "markdown",
    "id": "E3Xl2fVR5mkd"
   },
   "source": [
    "For this assignment, we've provided you with a set of pre-computed word\n",
    "embeddings. In particular, they are generated using the GloVe method that was\n",
    "mentioned in the slides and textbook readings. To avoid having to work with a\n",
    "huge amount of data, we've pre-selected only a\n",
    "subset of the full GloVe vocabulary and given it to you in a file (you can\n",
    "actually read the text file yourself to see what it looks like, if you're\n",
    "curious).\n",
    "\n",
    "We can load the embeddings into a Python object using a function we wrote\n",
    "(see util.py) and with the help of Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-e834f9b0-e9ab-4975-bab3-4c927bf64c7f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2706,
    "execution_start": 1619508857627,
    "id": "Ya7K0WAk5qzQ",
    "output_cleared": true,
    "source_hash": "bcfc1941",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Load in our embeddings from a text file\n",
    "embeddings = load_embeddings(\"data/glove.dataset_small.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each of the embeddings is a NumPy vector, and we can access them by simply\n",
    "looking up the word we are interested in (as a string) in the embeddings object.\n",
    "\n",
    "Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00029-0a4b0dc6-1319-427c-a241-b59d106c94f1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1619497150059,
    "id": "GRCNuGgk5sRA",
    "output_cleared": true,
    "source_hash": "87f61896",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Print out some of our embeddings\n",
    "words = ['the', 'good', 'red']\n",
    "\n",
    "for word in words:\n",
    "    print(\"{} : {}\".format(word, embeddings[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-cc0dfb7d-13ce-4575-9f03-f91b839b83da",
    "deepnote_cell_type": "markdown",
    "id": "npNN-WzK5uMG"
   },
   "source": [
    "We also added a special token, \"<unk\\>\", which represents any word that isn't in\n",
    "the embedding vocabulary (i.e. any word that we don't have an embedding for).\n",
    "The embedding for that token is just the average of the embeddings of all the\n",
    "other words.\n",
    "\n",
    "This will be important later, because there are plenty of words that we don't\n",
    "have embeddings for (you can try looking up a nonsense word in embeddings and\n",
    "see what happens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00031-5479adc4-c31d-484d-95d3-e8438c125c5e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1619497153647,
    "id": "WFNWajr15wDc",
    "output_cleared": true,
    "source_hash": "80a3fe75",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"<unk> embedding : {}\".format(embeddings[\"<unk>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-4c4b4c7c-e79b-4e5d-819c-e13db2b175f3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Now we have a way to translate our text data into a form that our neural\n",
    "network can use! Let's load up some data and do just that.\n",
    "\n",
    "The code below loads our dataset in from a file (see util.py for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-e9880d42-a75c-45df-a13f-6124c35e542b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1468,
    "execution_start": 1619508836599,
    "output_cleared": true,
    "source_hash": "1f06d4ea",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "train_examples, train_labels = load_dataset(\"data/train.csv\")\n",
    "dev_examples, dev_labels = load_dataset(\"data/dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00043-1ae2f230-dfb4-45f8-99ed-b28f7fa4b1ed",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Each element in train_examples is a Yelp review that we've preprocessed into a\n",
    "list of words. We've already taken out punctuation, segmented the words, and\n",
    "converted them all to lower case for you, to make them easier to work with.\n",
    "\n",
    "Each element in train_labels is the review's corresponding positive/negative\n",
    "label. As you probably know if you've ever used Yelp, ratings are normally from\n",
    "1-5 stars. However, in this case we've transformed the original star\n",
    "ratings into two labels (0 or 1) by collapsing 1 and 2-star reviews into the\n",
    "negative class (label 0) and 4 and 5-star reviews into the positive class\n",
    "(label 1). We ignore 3-star reviews, because they may be ambiguous and difficult\n",
    "for our classifier to handle.\n",
    "\n",
    "dev_examples and dev_labels are a separate development set of examples, but\n",
    "have the exact same format.\n",
    "\n",
    "We can examine a couple reviews and their ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00044-7461922c-3eaa-41ac-a02c-d3e9e1f27340",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1619508838826,
    "output_cleared": true,
    "source_hash": "6862c178",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "EXAMPLES_TO_PRINT = 5\n",
    "\n",
    "for text, label in zip(train_examples[:EXAMPLES_TO_PRINT],\n",
    "                       train_labels[:EXAMPLES_TO_PRINT]):\n",
    "    print(\"Review: {}\".format(\" \".join(text)))\n",
    "    print(\"Sentiment: {}\".format(label))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00045-893663f0-3d4d-42cc-b226-d31ce1c7968e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "To turn these words and labels into something we can work with, we'll need to\n",
    "do a little work.\n",
    "\n",
    "First, we need to convert each of the words in each review into word embeddings.\n",
    "But there is a catch: we might not have an embedding for every word in every\n",
    "review. In the event that we encounter an unknown word (that we don't have an\n",
    "embedding for), we can use the \"<unk\\>\" embedding as its embedding instead.\n",
    "You'll implement this as part of examples_to_array().\n",
    "\n",
    "Next, we need to handle the fact that reviews can be of varying lengths\n",
    "(different numbers of words), while our simple neural network can only take\n",
    "input of a single, fixed size. For example, in our simple neural network above,\n",
    "our inputs were all size 2 (2D points).\n",
    "\n",
    "We mentioned a similar issue to this in the modules in the context of neural\n",
    "language models. There are a couple of different ways to handle it, but in this\n",
    "case we will compress all the words (word embeddings) in each review into a\n",
    "single, fixed-size vector for the entire review. We can do this by taking the\n",
    "average of all of the word embeddings in the review, or alternatively taking\n",
    "the sum or the element-wise maximum. After doing this, each review will be\n",
    "represented by a single vector of the same size. You'll implement this as part\n",
    "of aggregate_embeddings() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-0d21fdbe-9336-4f51-90c1-4ce0cb7f2458",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1619508840946,
    "output_cleared": true,
    "source_hash": "bfcaebbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_embeddings(list_of_embeddings: List[np.ndarray],\n",
    "                         mode: str=\"mean\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Take a review in the form of a list of embedding vectors\n",
    "    and return a vector (of the same shape as a single embedding vector)\n",
    "    that represents the entire sentence.\n",
    "\n",
    "    You need to support 3 modes: \"mean\", \"sum\", and \"max\".\n",
    "\n",
    "    Args:\n",
    "        list_of_embeddings (List[np.ndarray]): A list of word embeddings, which\n",
    "                                                collectively represent a single\n",
    "                                                Yelp review. The list includes\n",
    "                                                one word embedding for each\n",
    "                                                word in the review. Each word\n",
    "                                                embedding is a vector of\n",
    "                                                shape (50,).\n",
    "        mode (str): How to aggregate all of the word embeddings in the review\n",
    "                    together. There are only 3 valid modes: mean, sum, and max.\n",
    "                    Mean and sum are self-explanatory. Max takes the\n",
    "                    element-wise max of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A single vector representing the entire Yelp review. It\n",
    "        should be of shape (50,), the same shape as a single word embedding.\n",
    "\n",
    "    HINT: For max, you can use np.amax().\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    ######################## YOUR CODE STARTS HERE #############################\n",
    "    \"\"\"\n",
    "    if mode == \"mean\":\n",
    "        aggregated = None\n",
    "    elif mode == \"sum\":\n",
    "        aggregated = None\n",
    "    elif mode == \"max\":\n",
    "        aggregated = None\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode: {}\".format(mode))\n",
    "    \"\"\"\n",
    "    ######################## YOUR CODE END HERE ################################\n",
    "    \"\"\"\n",
    "    return aggregated\n",
    "\n",
    "def examples_to_array(examples: List[List[str]],\n",
    "                      embeddings: gensim.models.KeyedVectors,\n",
    "                      mode: str=\"mean\"):\n",
    "    \"\"\"\n",
    "    TODO: Take a list of Yelp reviews, where each review is a list\n",
    "    of words, and convert it into an array of shape (num_examples,\n",
    "    embedding size) where each row is the aggregated embedding representing\n",
    "    that review.\n",
    "\n",
    "    Args:\n",
    "        examples (List[List[str]]): A list of examples, where each example is\n",
    "                                    a Yelp review. Each example/review is\n",
    "                                    represented as a list of words, where each\n",
    "                                    word is a separate string. This is the\n",
    "                                    first output of the load_dataset() function\n",
    "                                    above.\n",
    "        embeddings (gensim.models.KeyedVectors): The embeddings object loaded\n",
    "                                                earlier with load_embeddings().\n",
    "\n",
    "        mode (str): The mode argument to pass in to aggregate_embeddings().\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A single NumPy array containing the vector for each example\n",
    "                    (review) in the list of examples. Should be of shape\n",
    "                    (num_examples, 50).\n",
    "\n",
    "    HINT: You will need to handle words that are not in the vocabulary!\n",
    "    If a word is not in the vocabulary, then (word in embeddings) will\n",
    "    be False. In that case, you should use the \"<unk>\" embedding\n",
    "    as the embedding for the unknown word.\n",
    "\n",
    "    HINT: You first need to convert each word in a sentence into an\n",
    "    embedding, using embeddings. Then once you have a list of embeddings\n",
    "    (one for each word), use aggregate_embeddings() on each sentence to get\n",
    "    a single vector of shape (embedding_size,) per sentence. Then you\n",
    "    need to combine these vectors into an array of shape (num_examples,\n",
    "    embedding_size).\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ######################## YOUR CODE STARTS HERE #############################\n",
    "    \"\"\"\n",
    "    examples_array = None\n",
    "    \"\"\"\n",
    "    ######################## YOUR CODE END HERE ################################\n",
    "    \"\"\"\n",
    "    return examples_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00047-2abf4de2-48ca-486c-a6cb-fc3bc51b6509",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "You can test your implementation with this check (again, this is not a foolproof\n",
    "guarantee of correctness):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00048-a2211057-464f-4ac1-b4bb-83ed12f6cb92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1619508863998,
    "output_cleared": true,
    "source_hash": "848123eb",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "check_examples_to_array(examples_to_array, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-40f5937c-e4c4-4ada-b87e-ec61fb27555c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Fantastic! Let's finish up by pre-processing our entire dataset with the code\n",
    "that you wrote:\n",
    "\n",
    "__This may take a few seconds.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-3726aa27-a13c-4dc1-8666-457ae87e953b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11627,
    "execution_start": 1619508866473,
    "id": "IHjHbmKw50CD",
    "output_cleared": true,
    "source_hash": "1d5f358",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Note that we use expand_dims to change the shape of the labels from\n",
    "# (num_examples,) to (num_examples, 1) as our model requires.\n",
    "train_examples_array = examples_to_array(train_examples, embeddings)\n",
    "train_labels_array = np.expand_dims(np.array(train_labels), axis=1)\n",
    "\n",
    "dev_examples_array = examples_to_array(dev_examples, embeddings)\n",
    "dev_labels_array = np.expand_dims(np.array(dev_labels), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00034-996a205e-b71f-401e-bd38-d6d67d1004cc",
    "deepnote_cell_type": "markdown",
    "id": "U1nNK_Rz51qC"
   },
   "source": [
    "### TODO: Yelp Review Classification Model\n",
    "\n",
    "And now all we need to do is tweak our previous model slightly to work on our\n",
    "new problem. We'll need to do a couple of things:\n",
    "\n",
    "1. In our previous network, we applied the sigmoid function 3 times, once for\n",
    "each layer. This time, we'll use a different non-linear function (the __ReLU__\n",
    "function) for hidden layers, but still use sigmoid on our final output. This\n",
    "is slightly more realistic (ReLU is the more commonly used activation function\n",
    "in real-world neural networks, and should help our model learn better on this\n",
    "more difficult task).\n",
    "\n",
    "2. We'll use a slightly different training method (mini-batch gradient descent\n",
    "instead of gradient descent), because our dataset is much larger now and it\n",
    "will be difficult to do the calculations on the whole dataset at once.\n",
    "\n",
    "3. We'll use a slightly different way of initializing the weights, so that the\n",
    "initialization is better suited to our new ReLU activation functions.\n",
    "\n",
    "We've taken care of 2 and 3 for you, so your only task here will be\n",
    "implementing #1 and updating the forward and backward passes accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00057-d333c5fc-b733-4d65-844d-7819a9b15d2e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Some Helpful Information:\n",
    "\n",
    "__ReLU__: The ReLU (rectified linear unit) function is a simple non-linear\n",
    "function that we often use in neural networks. We described it in the\n",
    "modules/textbook. It is calculated as:\n",
    "\n",
    "$ReLU(x) = \\max(0, x)$\n",
    "\n",
    "Furthermore, the gradient of the relu function is:\n",
    "\n",
    "$\\frac{d}{dx} ReLU(x) = \\begin{cases}\n",
    "   0 &\\text{if } x < 0 \\\\\n",
    "   1 &\\text{if } x \\geq 0\n",
    "\\end{cases}$\n",
    "\n",
    "Note that when we apply the ReLU function to an array, we apply it to every\n",
    "element in the array separately. The same goes when calculating the gradient of\n",
    "ReLU. We compute it separately element-by-element (it has a 0\n",
    "for each element that is negative, and a 1 for each element that is\n",
    "non-negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-8e538535-5cc9-4a49-b3e0-28d7dee6b2d2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### The Math, Version 2\n",
    "\n",
    "To give you a hand with the implementation, just like we did above in Part 1,\n",
    "we'll write out the equations for the forward and backward passes. Note that\n",
    "these are slightly different now, because we've changed the activations for the\n",
    "hidden layers from sigmoid() to ReLU().\n",
    "\n",
    "For our modified network, the forward pass equations are:\n",
    "\n",
    "$z_1 = X W_1 + b_1$\n",
    "\n",
    "$a_1 = ReLU(z_1)$\n",
    "\n",
    "$z_2 = a_1 W_2 + b_2$\n",
    "\n",
    "$a_2 = ReLU(z_2)$\n",
    "\n",
    "$z_3 = a_2 W_3 + b_3$\n",
    "\n",
    "$a_3 = \\sigma(z_3)$\n",
    "\n",
    "The loss function is still the same (binary cross-entropy loss):\n",
    "\n",
    "$J = - ( y log(a_3) + (1 - y) log(1 - a_3) )$\n",
    "\n",
    "And the gradients computed during the backward pass are:\n",
    "\n",
    "$\\frac{dJ}{dz_3} = a_3 - y$\n",
    "\n",
    "$\\frac{dJ}{dW_3} = \\frac{1}{m} (a_{2}^T \\frac{dJ}{dz_3})$\n",
    "\n",
    "$\\frac{dJ}{db_3} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_3})_i$\n",
    "\n",
    "$\\frac{dJ}{da_2} = \\frac{dJ}{dz_3} W_3^T$\n",
    "\n",
    "$\\frac{dJ}{dz_2} = \\frac{dJ}{da_2} \\circ 1\\{a_2 > 0\\}$\n",
    "\n",
    "$\\frac{dJ}{dW_2} = \\frac{1}{m} (a_{1}^T \\frac{dJ}{dz_2})$\n",
    "\n",
    "$\\frac{dJ}{db_2} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_2})_i$\n",
    "\n",
    "$\\frac{dJ}{da_1} = \\frac{dJ}{dz_2} W_2^T$\n",
    "\n",
    "$\\frac{dJ}{dz_1} = \\frac{dJ}{da_1} \\circ 1\\{a_1 > 0\\}$\n",
    "\n",
    "$\\frac{dJ}{dW_1} = \\frac{1}{m} (X^T \\frac{dJ}{dz_1})$\n",
    "\n",
    "$\\frac{dJ}{db_1} = \\frac{1}{m} \\sum_{i=1}^m (\\frac{dJ}{dz_1})_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-617a352f-3788-4a7c-aaec-73718e1952ba",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1619508809426,
    "id": "WGLteCTl54uv",
    "output_cleared": true,
    "source_hash": "b185568d"
   },
   "outputs": [],
   "source": [
    "class YelpClassificationNeuralNetwork(StackedLogisticRegressionNetwork):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(YelpClassificationNeuralNetwork, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_weights(num_inputs: int, num_outputs: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        We overwrite the previous weight initialization approach with a slightly\n",
    "        different one that works better with our new ReLU activations (He\n",
    "        initialization).\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): The number of inputs to the layer.\n",
    "            num_outputs (int): The number of outputs (units) in the layer.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A fully initialized NumPy array of weights, of shape\n",
    "            (num_inputs, num_outputs)\n",
    "        \"\"\"\n",
    "        stddev = np.sqrt(2 /(num_inputs + num_outputs))\n",
    "        return np.random.normal(0, stddev, (num_inputs, num_outputs))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO: Implement the ReLU function.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): A NumPy array of any shape.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A new NumPy array where each element is the output\n",
    "                        of applying ReLU to the corresponding element of x.\n",
    "                        The shape of the output should be identical to the\n",
    "                        shape of x.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        relu_x = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE END HERE ############################\n",
    "        \"\"\"\n",
    "        return relu_x\n",
    "\n",
    "    def forward_pass(self, X: np.ndarray\n",
    "                     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        TODO: Calculate the outputs of each of the three layers in the model\n",
    "        using the inputs, weights, and biases.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The NumPy array of input examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple of three NumPy\n",
    "                arrays, corresponding to the output (after activation) of the\n",
    "                first layer ($a_1$), second layer ($a_2$), and third/output\n",
    "                layer ($a_3$).\n",
    "\n",
    "        HINT: his should look __exactly__ like what\n",
    "        you did in the \"stacked logistic regression\" model above.\n",
    "\n",
    "        The only difference is that you should replace the non-linear functions:\n",
    "        Use self.relu() for the first two, and self.sigmoid() for the final\n",
    "        output.\n",
    "\n",
    "        You can consult the equations above as you implement this.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        a_1 = None\n",
    "        a_2 = None\n",
    "        a_3 = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE END HERE ############################\n",
    "        \"\"\"\n",
    "        return a_1, a_2, a_3\n",
    "\n",
    "    def backward_pass(self,\n",
    "                      X: np.ndarray,\n",
    "                      y: np.ndarray,\n",
    "                      a_1: np.ndarray,\n",
    "                      a_2: np.ndarray,\n",
    "                      a_3: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        TODO: Use the data examples, labels, and model outputs to compute all of\n",
    "        the gradients with respect to the weights and biases.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The NumPy array of input examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "            y (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X. Has shape\n",
    "                            (num_examples, 1).\n",
    "            a_1 (np.ndarray): The output of the first logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "            a_2 (np.ndarray): The output of the second logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "            a_3 (np.ndarray): The output of the third logistic regression\n",
    "                              layer (after activation). Computed in\n",
    "                              forward_pass().\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: A dictionary of the gradients computed for\n",
    "                                    each weight and bias parameter. The keys\n",
    "                                    are strings, the values are NumPy arrays\n",
    "                                    (see the names of the keys below).\n",
    "\n",
    "        HINT: This should look exactly like what we did for the stacked logistic\n",
    "        regression model above. The only difference is that we should replace the\n",
    "        sigmoid gradients (i.e. in dz_2 and dz_1) with the gradient of the\n",
    "        ReLU function.\n",
    "\n",
    "        You can consult the equations above as you implement this.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE STARTS HERE #########################\n",
    "        \"\"\"\n",
    "        dW_3 = None\n",
    "        db_3 = None\n",
    "        dW_2 = None\n",
    "        db_2 = None\n",
    "        dW_1 = None\n",
    "        db_1 = None\n",
    "        \"\"\"\n",
    "        ######################## YOUR CODE END HERE ############################\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the gradients\n",
    "        gradients = {\n",
    "          \"W_3\": dW_3,\n",
    "          \"b_3\": db_3,\n",
    "          \"W_2\": dW_2,\n",
    "          \"b_2\": db_2,\n",
    "          \"W_1\": dW_1,\n",
    "          \"b_1\": db_1\n",
    "        }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def train_batch(self,\n",
    "                    X_train: np.ndarray,\n",
    "                    y_train: np.ndarray,\n",
    "                    X_dev: np.ndarray,\n",
    "                    y_dev: np.ndarray,\n",
    "                    batch_size: int=1000,\n",
    "                    learning_rate: float=0.001,\n",
    "                    num_epochs: int=100,\n",
    "                    print_every: int=10,\n",
    "                    verbose: bool=True):\n",
    "        \"\"\"\n",
    "        Trains the model with batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): The NumPy array of training examples, of shape\n",
    "                            (num_examples, input_size).\n",
    "            y_train (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X_train. Has shape\n",
    "                            (num_examples, 1). Each element is either 0.0 or\n",
    "                            1.0.\n",
    "            X_dev (np.ndarray): The NumPy array of dev examples. Properties the\n",
    "                                same as X_train.\n",
    "            y_dev (np.ndarray): The NumPy array of true labels for each\n",
    "                            example in X_dev. Properties the same as y_train.\n",
    "            batch_size (int): The number of examples in each batch during\n",
    "                               mini-batch gradient descent.\n",
    "            learning_rate (np.ndarray): Step size parameter for gradient\n",
    "                                descent.\n",
    "            num_epochs (int): The number of epochs (full passes through the\n",
    "                                provided training data) to train the model for.\n",
    "            print_every (int): Number of epochs to wait between printouts of the\n",
    "                                training loss and accuracy during training.\n",
    "            verbose (bool): Whether to print any training output.\n",
    "\n",
    "        This is just a slightly modified version of the train() function for\n",
    "        our earlier model.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            num_batches = 0\n",
    "            for batch_start in range(0, X_train.shape[0], batch_size):\n",
    "                batch_end = min(X_train.shape[0], batch_start + batch_size)\n",
    "                train_X_batch = X_train[batch_start:batch_end]\n",
    "                train_y_batch = y_train[batch_start:batch_end]\n",
    "                a_1, a_2, a_3 = self.forward_pass(train_X_batch)\n",
    "                gradients = self.backward_pass(train_X_batch, train_y_batch, a_1,\n",
    "                                               a_2, a_3)\n",
    "\n",
    "                for name, grad in gradients.items():\n",
    "                    self.variables[name] -= learning_rate * grad\n",
    "\n",
    "                total_loss += self.loss(a_3, train_y_batch)\n",
    "                total_correct += np.sum(\n",
    "                    self.predict(train_X_batch) == train_y_batch)\n",
    "                num_batches += 1\n",
    "            if verbose and epoch % print_every == 0:\n",
    "                train_loss = total_loss / num_batches\n",
    "                train_accuracy = total_correct / X_train.shape[0]\n",
    "                dev_loss = self.loss(self.forward_pass(X_dev)[2], y_dev)\n",
    "                dev_accuracy = np.mean(self.predict(X_dev) == y_dev)\n",
    "                print(\"Epoch: {} | Train Loss: {} | Train Acc: {} | Dev Loss: {} | Dev Acc: {}\".format(epoch,\n",
    "                                                                train_loss,\n",
    "                                                              train_accuracy,\n",
    "                                                              dev_loss,\n",
    "                                                              dev_accuracy))\n",
    "        print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00059-5b86d472-3f48-4725-8c4e-25579e2a7f74",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Here are some more sanity checks to make sure you've gotten everything working.\n",
    "As usual, they don't guarantee your solution is correct, but they're a helpful\n",
    "way to check if it's wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00062-e85d62ce-ad98-41d9-b0a4-3734af4300c3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1619508950155,
    "output_cleared": true,
    "source_hash": "68e9fd5a",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "check_forward_pass_yelp(YelpClassificationNeuralNetwork, train_examples_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00058-2c7d1cc8-2fbf-4bbe-a7c3-42cb045b9363",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1619509301393,
    "output_cleared": true,
    "source_hash": "360d2b2b",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "check_backward_pass_yelp(YelpClassificationNeuralNetwork,\n",
    "                         train_examples_array, train_labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00059-e42e4aa2-1233-4850-8326-b2319f5c3ef9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "If you've passed all the sanity checks above, your model is most likely\n",
    "working correctly.\n",
    "\n",
    "Let's train it!\n",
    "\n",
    "__NOTE:__ Training this model may take some time. We recommend taking a break\n",
    "while it runs, or doing something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-6dd0ba0a-66a5-417a-8d15-a0d4cab83be9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 327393,
    "execution_start": 1619499654992,
    "id": "7_7O_NPy6CZ_",
    "output_cleared": true,
    "source_hash": "382f731b",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "model = YelpClassificationNeuralNetwork(50, 100, 100)\n",
    "\n",
    "model.train_batch(\n",
    "    train_examples_array,\n",
    "    train_labels_array,\n",
    "    dev_examples_array,\n",
    "    dev_labels_array,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=5000,\n",
    "    num_epochs=500,\n",
    "    print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00059-f3790d10-c2d3-4ed0-894e-c4cb182230c6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Evaluating our Model\n",
    "\n",
    "You should find that your classifier is not perfect,\n",
    "but it's not too shabby either!\n",
    "\n",
    "To give a rough ballpark estimate of what you should expect to see,\n",
    "our implementation was able to achieve a\n",
    "training and dev accuracy of around the low to mid 80s percent using the exact training settings above.\n",
    "\n",
    "As part of the evaluation for this part, the autograder will train up your model implementation (using the same\n",
    "training settings above) and evaluate your performance on the dev set, as well as a separate hidden test set.\n",
    "\n",
    "You will be given up to 3 points for dev performance and 3 points for test performance. On both datasets, you will get 1 point if you scare at least 50% accuracy (this is a freebie, as in a binary classification task with balanced classes, guessing all class 1 will give you 50% accuracy). You will gain a further 1 point for scoring at least 80% accuracy. And you will gain a final 1 point for scoring above 84% accuracy.\n",
    "\n",
    "We've observed that performance on dev and test are very similar, as one would expect, so if you are able to meet the dev thresholds when training locally, you are almost guaranteed to get the 3 points for test as well.\n",
    "\n",
    "__NOTE:__ As part of evaluating your model, the autograder will train it for 500 epochs. This will take a bit of time to run, so we don't recommend submitting rapid-fire to the autograder to test out every small change and fix you make to your model. Instead, you should try to test each part locally (the sanity checks should help with that), and verify that your model trains and performs well locally before submitting to Gradescope.\n",
    "\n",
    "Now, with that out of the way, and now that we've done the hard part of implementing and training a neural network\n",
    "on a real language task, this is our chance to play around with it a bit and\n",
    "see what it can do:\n",
    "\n",
    "The function below takes in the trained model, the preprocessing\n",
    "examples_to_array function, the embeddings object, and an example \"review\"\n",
    "that you wrote and returns the model's prediction about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00068-346c05e8-7624-4915-b2e7-c7b237a034e1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1619509458587,
    "output_cleared": true,
    "source_hash": "659a8f5e",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "evaluate(model, examples_to_array, embeddings, \"The food was fantastic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00064-0f9f2e8f-bbbe-4e30-a764-28b894cd11bf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1619509460555,
    "output_cleared": true,
    "source_hash": "a8b84228",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "evaluate(model, examples_to_array, embeddings, \"I did not like it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00069-88427d70-3bc3-409d-b3b6-6c71233a3402",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "We encourage you to play around with your trained classifier and\n",
    "try out different sentences. When does it do well? When does it fail? What\n",
    "weaknesses do you see in our approach? Does it seem similar to the logistic\n",
    "regression and Naive Bayes models you implemented earlier? Or different?\n",
    "\n",
    "__[Try this:]__ Although we will only grade your model's\n",
    "performance/implementation using\n",
    "the \"mean\" aggregation method, we encourage you to try re-processing the data\n",
    "and re-training your model with the other aggregation strategies (sum or max).\n",
    "This should only take a couple minutes at most.\n",
    "\n",
    "Once you do that, consider the following questions:\n",
    "How do the different settings affect your results? Do any of the other methods\n",
    "seem to perform better/worse than using the mean? Can you explain the results\n",
    "that you're seeing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Interpreting our Results, and Final Thoughts\n",
    "\n",
    "As a final note, one thing you may have noticed about this model, compared to\n",
    "the logistic regression classifier you implemented in PA2, is that it's no\n",
    "longer possible to easily interpret the weights in the model. In PA2, we\n",
    "examined the weights for each input feature to our logistic regression model,\n",
    "and those told us something about what our model thinks are good indicators\n",
    "of each label. A large positive weight meant the feature was associated with\n",
    "a positive label, while a large negative weight meant the feature was associated\n",
    "with a negative label.\n",
    "\n",
    "However, in this case there's no longer an easy way to do this! Why's that?\n",
    "First, it's no longer clear what each input feature even means, because\n",
    "they come from dense word vectors/embeddings (what does the 5th or 48th element\n",
    "in one of our word vectors really represent?).\n",
    "\n",
    "And secondly, even if we knew what each input feature was, the multi-layer\n",
    "nature of our model means there's no longer a clear and interpretable connection\n",
    "between our inputs and our outputs. Note that we have no idea what sort of\n",
    "features or representations the network is producing in its hidden (middle)\n",
    "layers or why. So even if we looked at the weight from\n",
    "input feature #5 to hidden unit #12 in the first hidden layer, there's no easy\n",
    "way to say exactly what this \"means\" in terms of the relationship between input\n",
    "feature #5 and our prediction. This problem only becomes more serious as neural\n",
    "networks become deeper (more layers) and more complex: as the number of\n",
    "weights becomes extremely large, it becomes almost impossible to fully\n",
    "understand the complicated relationships between the inputs, weights, and\n",
    "outputs.\n",
    "\n",
    "This is not just a mathematical or theoretical observation! It's a serious\n",
    "issue that comes up often when applying neural network models in the real world,\n",
    "especially to language. We often want to use the information we\n",
    "extract from language to make important decisions, but when these decisions have\n",
    "an impact on the lives of real people, it's extremely important that our models\n",
    "not only make the right decisions, but that we understand\n",
    "__why__ these decisions were made and where they could be wrong or even\n",
    "unintentionally harmful or biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To mention just a couple of serious real-world examples, neural networks have been trained that [inadvertently discriminate\n",
    "against women in hiring practices](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "or [use zip code as a proxy for race](https://engineering.cmu.edu/news-events/news/2018/12/11-datta-proxies.html)\n",
    "because the engineers building the systems were not able to fully understand\n",
    "which features in the training data the network was using, or how it was\n",
    "reaching its decisions.\n",
    "\n",
    "Based on what you're learned so far about neural networks and NLP, what\n",
    "are some other potential downsides to networks being opaque/uninterpretable?\n",
    "Do you have any ideas for how we could increase neural network interpretability?\n",
    "Please respond in 1-2 paragraphs.\n",
    "\n",
    "Write your response in a string that is returned by the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def neural_network_interpretability_response():\n",
    "    \"\"\"\n",
    "    TODO: Write your response below.\n",
    "    \"\"\"\n",
    "    return \"[TODO]: YOUR RESPONSE HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "CONGRATS! You're done!\n",
    "\n",
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00053-55919144-e9a1-4ec6-b723-145420e564a2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "id": "vAsNmB39ynRB",
    "output_cleared": true,
    "source_hash": "fc295dda",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa5.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa5.ipynb deps/\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-005f5170-ce17-49d0-b17b-807bf1047813",
    "deepnote_cell_type": "markdown",
    "id": "MlR7fvySysAL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa5.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa5.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the pa3 Triage assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cs124_pa_5_ramp_up.ipynb",
   "provenance": []
  },
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5e42aa6c-1fdf-4499-ae69-7de303b68aae",
  "kernelspec": {
   "display_name": "Python [conda env:cs124] *",
   "language": "python",
   "name": "conda-env-cs124-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
